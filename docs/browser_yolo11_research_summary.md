# Browser-Based YOLO11 Inference Research - Executive Summary

**Research Date**: October 16, 2025
**Project**: Deeper Darts - Web-Based Dart Detection
**Status**: ‚úÖ Research Complete

---

## üéØ Key Findings

### ‚úÖ FEASIBILITY: YES
Browser-based YOLO11 inference on iPhone is **fully feasible** with acceptable performance for real-time applications.

### üìä Expected Performance
- **Desktop Browser**: 15-25 FPS
- **iPhone 13+ (newer)**: 10-15 FPS
- **iPhone 12- (older)**: 5-10 FPS

---

## üöÄ RECOMMENDED APPROACH

### Best Technology Stack
1. **Model**: YOLOv11n (nano) - 5.5 MB
2. **Runtime**: ONNX Runtime Web with WebAssembly SIMD
3. **Input Size**: 416x416 (balance of speed/accuracy)
4. **Deployment**: HTTPS-enabled hosting (required for camera access)

### Why ONNX Runtime Web?
- ‚úÖ Up to 3x faster than native PyTorch (CPU)
- ‚úÖ 3.4x acceleration with SIMD + multi-threading
- ‚úÖ Smaller bundle size than TensorFlow.js
- ‚úÖ Single-step conversion (PyTorch ‚Üí ONNX)
- ‚úÖ WebGL/WebGPU acceleration available
- ‚úÖ ~220ms average inference time

---

## üì± iOS SAFARI REQUIREMENTS

### Camera Access (getUserMedia)
- **iOS 13.5.1+**: ‚úÖ Full PWA standalone support
- **iOS 11-13.4**: ‚ö†Ô∏è Requires browser mode (not standalone)
- **HTTPS Required**: Mandatory for camera access (except localhost)

### PWA Configuration
```json
{
  "name": "YOLO Detector",
  "display": "standalone",  // Works iOS 13.5.1+
  "start_url": "/",
  "icons": [...]
}
```

For iOS <13.5.1, use `"display": "browser"` or device-specific manifest workaround.

---

## üîß IMPLEMENTATION QUICKSTART

### 1. Export Your Model
```python
from ultralytics import YOLO

model = YOLO('best.pt')  # Your trained model
model.export(
    format='onnx',
    imgsz=416,        # 416x416 for speed
    half=False,       # FP32 for compatibility
    simplify=True,
    dynamic=False
)
# Output: best.onnx
```

### 2. Setup Web Project
```bash
npm create vite@latest yolo-detector -- --template react
cd yolo-detector
npm install onnxruntime-web opencv.js
```

### 3. Basic Detection Code
```javascript
import * as ort from 'onnxruntime-web';

// Load model
const session = await ort.InferenceSession.create('/models/best.onnx', {
  executionProviders: ['webgl', 'wasm'],
  graphOptimizationLevel: 'all'
});

// Get camera
const stream = await navigator.mediaDevices.getUserMedia({
  video: { facingMode: 'environment', width: 1280, height: 720 }
});

// Run inference
const inputTensor = preprocessImage(videoFrame);
const outputs = await session.run({ images: inputTensor });
const boxes = parseYOLOOutput(outputs.output0.data);
drawBoundingBoxes(boxes);
```

**Full implementation code available in comprehensive research document.**

---

## ‚ö° OPTIMIZATION STRATEGIES

### Model Optimization
1. **Use YOLOv11n**: 5.5 MB, 6.5B ops, 39.5% mAP COCO
2. **Lower Resolution**: 320x320 or 416x416 (vs 640x640)
3. **Quantization**: FP16 (50% smaller) or INT8 (75% smaller)

### Runtime Optimization
1. **Frame Skipping**: Process every 2nd or 3rd frame
2. **WebGL Acceleration**: 2-3x speedup on supported devices
3. **Web Workers**: Offload processing to background thread
4. **Service Worker**: Cache model for offline use

### iOS-Specific
1. **Detect Device**: Adjust settings for older iPhones
2. **Battery Mode**: Pause when app backgrounded
3. **Memory Management**: Dispose tensors after use

---

## üì¶ MODEL SIZE COMPARISON

| Model | Size | Operations | mAP (COCO) | Best For |
|-------|------|------------|------------|----------|
| YOLO Nano | 4.0 MB | 4.57B | 69.1% VOC | Ultra-lightweight |
| YOLOv11n | 5.5 MB | 6.5B | 39.5% | **Recommended** |
| YOLOv8n | 6.2 MB | 8.7B | 37.3% | Alternative |
| Tinier-YOLO | 8.9 MB | ~10B | 65.7% VOC | Embedded |

---

## üõ†Ô∏è REAL-WORLD IMPLEMENTATIONS

### GitHub Examples
1. **akbartus/Yolov8-Object-Detection-on-Browser**
   - ONNX + TensorFlow.js versions
   - Live demo: https://yolov8-object-detection.glitch.me/
   - Model: YOLOv8n (416x416)

2. **PyImageSearch Tutorial**
   - ONNX Runtime Web + Next.js + React
   - Complete preprocessing pipeline
   - Average: 220ms inference

3. **ModelDepot/tfjs-yolo-tiny**
   - TensorFlow.js implementation
   - Tiny YOLO variant
   - Simple webcam API

---

## ‚ö†Ô∏è KEY CHALLENGES & SOLUTIONS

### Challenge 1: iOS Camera Access
**Problem**: getUserMedia doesn't work in PWA standalone mode (iOS <13.5.1)

**Solutions**:
- ‚úÖ Target iOS 13.5.1+ (majority of users)
- ‚úÖ Use `display: "browser"` in manifest
- ‚úÖ Device-specific manifest switching
- ‚úÖ Fallback to `<input capture="environment">`

### Challenge 2: Performance on Older iPhones
**Problem**: <5 FPS on iPhone 11 and older

**Solutions**:
- ‚úÖ Use YOLO Nano (4 MB) instead of YOLOv11n
- ‚úÖ Reduce to 320x320 input
- ‚úÖ Frame skipping (process every 3rd frame)
- ‚úÖ Hybrid approach: fallback to cloud inference

### Challenge 3: Model Download Size
**Problem**: 10-20 MB initial download

**Solutions**:
- ‚úÖ Progressive loading with splash screen
- ‚úÖ Service Worker caching
- ‚úÖ Lazy load model after UI renders
- ‚úÖ Compress with gzip/brotli

---

## üéØ DEPLOYMENT PHASES

### Phase 1: MVP (2-3 days)
- ‚úÖ Export YOLOv11n to ONNX
- ‚úÖ Basic HTML/JS camera app
- ‚úÖ HTTPS deployment
- **Target**: 8-12 FPS on iPhone 13+

### Phase 2: Optimization (3-5 days)
- ‚úÖ WebGL acceleration
- ‚úÖ Frame skipping
- ‚úÖ Error handling + loading states
- **Target**: 12-18 FPS

### Phase 3: PWA (2-3 days)
- ‚úÖ Progressive Web App setup
- ‚úÖ Offline support
- ‚úÖ Install prompts
- **Target**: Native-like UX

### Phase 4: Production (ongoing)
- ‚úÖ Analytics + A/B testing
- ‚úÖ Multiple model support
- ‚úÖ Cloud inference fallback

---

## üìö ESSENTIAL RESOURCES

### Documentation
- **Ultralytics YOLO11**: https://docs.ultralytics.com/models/yolo11/
- **ONNX Runtime Web**: https://onnxruntime.ai/docs/tutorials/web/
- **Export Guide**: https://docs.ultralytics.com/modes/export/

### Code Examples
- **Browser YOLO (ONNX)**: https://github.com/akbartus/Yolov8-Object-Detection-on-Browser
- **PyImageSearch Tutorial**: https://pyimagesearch.com/2025/07/28/run-yolo-model-in-the-browser-with-onnx-webassembly-and-next-js/
- **Tiny YOLO (TF.js)**: https://github.com/ModelDepot/tfjs-yolo-tiny

### Benchmarks
- **YOLO Benchmarking**: https://docs.ultralytics.com/modes/benchmark/
- **iPhone Performance**: https://github.com/ultralytics/yolov5/issues/1276

---

## üö® CRITICAL REQUIREMENTS

### Must-Have
- ‚úÖ HTTPS deployment (required for camera)
- ‚úÖ iOS 13.5.1+ for standalone PWA
- ‚úÖ Model size <20 MB
- ‚úÖ Target 10+ FPS on iPhone 13
- ‚úÖ Graceful degradation for older devices

### Nice-to-Have
- ‚≠ê WebGL/WebGPU acceleration
- ‚≠ê Offline mode with Service Worker
- ‚≠ê Cloud inference fallback
- ‚≠ê A/B testing infrastructure

---

## üí° ALTERNATIVES

### If Browser Performance Insufficient

**Hybrid Approach**: Local + Cloud
```javascript
if (deviceFPS < 5) {
  // Fallback to cloud inference
  useCloudInference();
} else {
  // Use local browser inference
  useLocalInference();
}
```

**Pure Cloud Approach**: WebSocket streaming
- Server with GPU runs inference
- Client streams camera frames
- ~50-100ms network latency + inference
- Consistent 30+ FPS across all devices
- Privacy trade-off: frames sent to server

---

## ‚úÖ CONCLUSION

**Recommendation**: Proceed with ONNX Runtime Web implementation

**Expected Outcome**:
- ‚úÖ Real-time detection on modern iPhones (10-15 FPS)
- ‚úÖ Acceptable performance on older devices (5-10 FPS)
- ‚úÖ No server costs (client-side processing)
- ‚úÖ Privacy-preserving (data stays on device)
- ‚úÖ Works offline after initial model download

**Risk Mitigation**:
- Start with YOLOv11n for best balance
- Implement device detection and adaptive settings
- Provide cloud fallback for poor performance
- Monitor real-world FPS via analytics

**Next Steps**:
1. Export your trained `best.pt` model to ONNX
2. Implement basic browser detection (Phase 1)
3. Test on target iPhone devices
4. Iterate based on actual performance

---

**For complete technical details, code examples, and implementation guide, see the full research document stored in memory: `browser_yolo_research`**

**Memory ID**: `ba079cef-575d-4a28-ab5a-ed25737801fd`
**Storage**: `.swarm/memory.db` (ReasoningBank with semantic search enabled)
